{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd71ba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_summariser import summarise_url\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "url=\"https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f75750b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT\n",
    "model='gpt-4.1-nano'\n",
    "client = OpenAI(api_key=api_key)\n",
    "pdf_summary = summarise_url(client, url, model=model)\n",
    "print(pdf_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefc513e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db48ef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_summary = {'response_id': 'resp_0fc9878f2d45213200693ba97cfd288193a873b26c0e7af002',\n",
    " 'model': 'gpt-4.1-nano-2025-04-14',\n",
    " 'summary': 'The document titled \"The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity\" presents a comprehensive analysis of Large Reasoning Models (LRMs) and their reasoning capabilities across various puzzle environments. The key points, important details, and conclusions are summarized below:\\n\\n### Key Points:\\n- **Objective:** To systematically investigate the reasoning capabilities and limitations of frontier LRMs using controllable puzzle environments that allow precise manipulation of problem complexity.\\n- **Methodology:** Use of four puzzle environments (Tower of Hanoi, Checker Jumping, River Crossing, Blocks World) to evaluate models\\' reasoning processes, internal traces, and performance across different complexity levels.\\n- **Models Analyzed:** Several models including Claude-3.7-Sonnet (thinking/non-thinking), DeepSeek-R1/V3, and o3-mini, with access to reasoning traces.\\n- **Evaluation Approach:** Focus on both final answer accuracy and internal reasoning traces, including solution correctness, reasoning effort, and failure analysis.\\n\\n### Important Details:\\n- **Findings on Performance:**\\n  - LRMs fail to develop generalizable reasoning beyond certain complexity thresholds, with performance collapsing to zero at high complexity.\\n  - A counterintuitive scaling limit was observed: reasoning effort (tokens used for thinking) initially increases with complexity but then decreases despite increasing problem difficulty.\\n  - Three reasoning regimes identified:\\n    1. Low complexity: Non-thinking models outperform reasoning models.\\n    2. Medium complexity: Reasoning models show an advantage.\\n    3. High complexity: Both models collapse.\\n- **Analysis of Reasoning Traces:**\\n  - Models often overthink in simple problems, exploring incorrect solutions early.\\n  - In moderate problems, correct solutions emerge later, indicating extensive exploration.\\n  - In complex problems, models fail to find correct solutions, often fixating on early wrong answers.\\n- **Limitations in Exact Computation:**\\n  - LRMs struggle with explicit algorithms and exhibit inconsistent reasoning across puzzles.\\n  - Providing explicit algorithms (e.g., Tower of Hanoi solution) does not significantly improve performance.\\n- **Behavioral Insights:**\\n  - Models show non-monotonic failure patterns, with errors occurring at different points in the solution sequence depending on problem size.\\n  - Failure move distributions suggest models are more unstable and prone to inconsistent reasoning at higher complexities.\\n- **Scaling and Complexity:**\\n  - Compositional depth (number of moves) scales exponentially or quadratically with problem size, depending on the puzzle.\\n  - Performance correlates negatively with compositional depth, but this relationship varies across puzzle types.\\n- **Open Questions:**\\n  - Why do models reduce reasoning effort at high complexity?\\n  - Can models generate solutions with explicit algorithms effectively?\\n  - Are current evaluation paradigms sufficient to understand reasoning capabilities?\\n\\n### Conclusions:\\n- **Fundamental Limitations:** Despite sophisticated self-reflection mechanisms, current LRMs do not exhibit robust, generalizable reasoning beyond moderate complexity.\\n- **Scaling Barriers:** There are inherent scaling limits in reasoning effort and accuracy, with models failing to utilize additional compute effectively at high complexity.\\n- **Implications:** The findings challenge assumptions about the reasoning capabilities of LRMs and suggest the need for new approaches, including better evaluation methods and possibly hybrid symbolic-neural systems.\\n- **Future Directions:** Further research is needed to understand the symbolic manipulation capabilities, improve reasoning robustness, and explore environments that enable controlled experimentation.\\n\\n### Limitations:\\n- The study is limited to controlled puzzle environments, which may not fully capture real-world reasoning complexity.\\n- Use of black-box API models restricts internal analysis.\\n- Precise validation relies on deterministic puzzle simulators, which may not generalize to less structured domains.\\n\\nThis work provides critical insights into the current state of reasoning models, highlighting their strengths, weaknesses, and fundamental barriers to scalable, general reasoning.'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecb229d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The document titled \"The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity\" presents a comprehensive analysis of Large Reasoning Models (LRMs) and their reasoning capabilities across various puzzle environments. The key points, important details, and conclusions are summarized below:\\n\\n### Key Points:\\n- **Objective:** To systematically investigate the reasoning capabilities and limitations of frontier LRMs using controllable puzzle environments that allow precise manipulation of problem complexity.\\n- **Methodology:** Use of four puzzle environments (Tower of Hanoi, Checker Jumping, River Crossing, Blocks World) to evaluate models\\' reasoning processes, internal traces, and performance across different complexity levels.\\n- **Models Analyzed:** Several models including Claude-3.7-Sonnet (thinking/non-thinking), DeepSeek-R1/V3, and o3-mini, with access to reasoning traces.\\n- **Evaluation Approach:** Focus on both final answer accuracy and internal reasoning traces, including solution correctness, reasoning effort, and failure analysis.\\n\\n### Important Details:\\n- **Findings on Performance:**\\n  - LRMs fail to develop generalizable reasoning beyond certain complexity thresholds, with performance collapsing to zero at high complexity.\\n  - A counterintuitive scaling limit was observed: reasoning effort (tokens used for thinking) initially increases with complexity but then decreases despite increasing problem difficulty.\\n  - Three reasoning regimes identified:\\n    1. Low complexity: Non-thinking models outperform reasoning models.\\n    2. Medium complexity: Reasoning models show an advantage.\\n    3. High complexity: Both models collapse.\\n- **Analysis of Reasoning Traces:**\\n  - Models often overthink in simple problems, exploring incorrect solutions early.\\n  - In moderate problems, correct solutions emerge later, indicating extensive exploration.\\n  - In complex problems, models fail to find correct solutions, often fixating on early wrong answers.\\n- **Limitations in Exact Computation:**\\n  - LRMs struggle with explicit algorithms and exhibit inconsistent reasoning across puzzles.\\n  - Providing explicit algorithms (e.g., Tower of Hanoi solution) does not significantly improve performance.\\n- **Behavioral Insights:**\\n  - Models show non-monotonic failure patterns, with errors occurring at different points in the solution sequence depending on problem size.\\n  - Failure move distributions suggest models are more unstable and prone to inconsistent reasoning at higher complexities.\\n- **Scaling and Complexity:**\\n  - Compositional depth (number of moves) scales exponentially or quadratically with problem size, depending on the puzzle.\\n  - Performance correlates negatively with compositional depth, but this relationship varies across puzzle types.\\n- **Open Questions:**\\n  - Why do models reduce reasoning effort at high complexity?\\n  - Can models generate solutions with explicit algorithms effectively?\\n  - Are current evaluation paradigms sufficient to understand reasoning capabilities?\\n\\n### Conclusions:\\n- **Fundamental Limitations:** Despite sophisticated self-reflection mechanisms, current LRMs do not exhibit robust, generalizable reasoning beyond moderate complexity.\\n- **Scaling Barriers:** There are inherent scaling limits in reasoning effort and accuracy, with models failing to utilize additional compute effectively at high complexity.\\n- **Implications:** The findings challenge assumptions about the reasoning capabilities of LRMs and suggest the need for new approaches, including better evaluation methods and possibly hybrid symbolic-neural systems.\\n- **Future Directions:** Further research is needed to understand the symbolic manipulation capabilities, improve reasoning robustness, and explore environments that enable controlled experimentation.\\n\\n### Limitations:\\n- The study is limited to controlled puzzle environments, which may not fully capture real-world reasoning complexity.\\n- Use of black-box API models restricts internal analysis.\\n- Precise validation relies on deterministic puzzle simulators, which may not generalize to less structured domains.\\n\\nThis work provides critical insights into the current state of reasoning models, highlighting their strengths, weaknesses, and fundamental barriers to scalable, general reasoning.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34a40217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## PDF Summary\n",
       "\n",
       "The document titled \"The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity\" presents a comprehensive analysis of Large Reasoning Models (LRMs) and their reasoning capabilities across various puzzle environments. The key points, important details, and conclusions are summarized below:\n",
       "\n",
       "### Key Points:\n",
       "- **Objective:** To systematically investigate the reasoning capabilities and limitations of frontier LRMs using controllable puzzle environments that allow precise manipulation of problem complexity.\n",
       "- **Methodology:** Use of four puzzle environments (Tower of Hanoi, Checker Jumping, River Crossing, Blocks World) to evaluate models' reasoning processes, internal traces, and performance across different complexity levels.\n",
       "- **Models Analyzed:** Several models including Claude-3.7-Sonnet (thinking/non-thinking), DeepSeek-R1/V3, and o3-mini, with access to reasoning traces.\n",
       "- **Evaluation Approach:** Focus on both final answer accuracy and internal reasoning traces, including solution correctness, reasoning effort, and failure analysis.\n",
       "\n",
       "### Important Details:\n",
       "- **Findings on Performance:**\n",
       "  - LRMs fail to develop generalizable reasoning beyond certain complexity thresholds, with performance collapsing to zero at high complexity.\n",
       "  - A counterintuitive scaling limit was observed: reasoning effort (tokens used for thinking) initially increases with complexity but then decreases despite increasing problem difficulty.\n",
       "  - Three reasoning regimes identified:\n",
       "    1. Low complexity: Non-thinking models outperform reasoning models.\n",
       "    2. Medium complexity: Reasoning models show an advantage.\n",
       "    3. High complexity: Both models collapse.\n",
       "- **Analysis of Reasoning Traces:**\n",
       "  - Models often overthink in simple problems, exploring incorrect solutions early.\n",
       "  - In moderate problems, correct solutions emerge later, indicating extensive exploration.\n",
       "  - In complex problems, models fail to find correct solutions, often fixating on early wrong answers.\n",
       "- **Limitations in Exact Computation:**\n",
       "  - LRMs struggle with explicit algorithms and exhibit inconsistent reasoning across puzzles.\n",
       "  - Providing explicit algorithms (e.g., Tower of Hanoi solution) does not significantly improve performance.\n",
       "- **Behavioral Insights:**\n",
       "  - Models show non-monotonic failure patterns, with errors occurring at different points in the solution sequence depending on problem size.\n",
       "  - Failure move distributions suggest models are more unstable and prone to inconsistent reasoning at higher complexities.\n",
       "- **Scaling and Complexity:**\n",
       "  - Compositional depth (number of moves) scales exponentially or quadratically with problem size, depending on the puzzle.\n",
       "  - Performance correlates negatively with compositional depth, but this relationship varies across puzzle types.\n",
       "- **Open Questions:**\n",
       "  - Why do models reduce reasoning effort at high complexity?\n",
       "  - Can models generate solutions with explicit algorithms effectively?\n",
       "  - Are current evaluation paradigms sufficient to understand reasoning capabilities?\n",
       "\n",
       "### Conclusions:\n",
       "- **Fundamental Limitations:** Despite sophisticated self-reflection mechanisms, current LRMs do not exhibit robust, generalizable reasoning beyond moderate complexity.\n",
       "- **Scaling Barriers:** There are inherent scaling limits in reasoning effort and accuracy, with models failing to utilize additional compute effectively at high complexity.\n",
       "- **Implications:** The findings challenge assumptions about the reasoning capabilities of LRMs and suggest the need for new approaches, including better evaluation methods and possibly hybrid symbolic-neural systems.\n",
       "- **Future Directions:** Further research is needed to understand the symbolic manipulation capabilities, improve reasoning robustness, and explore environments that enable controlled experimentation.\n",
       "\n",
       "### Limitations:\n",
       "- The study is limited to controlled puzzle environments, which may not fully capture real-world reasoning complexity.\n",
       "- Use of black-box API models restricts internal analysis.\n",
       "- Precise validation relies on deterministic puzzle simulators, which may not generalize to less structured domains.\n",
       "\n",
       "This work provides critical insights into the current state of reasoning models, highlighting their strengths, weaknesses, and fundamental barriers to scalable, general reasoning."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(f\"## PDF Summary\\n\\n{pdf_summary['summary']}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fb9edbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://arxiv.org/pdf/2511.08042\"\n",
    "url = \"https://arxiv.org/pdf/2512.05156\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f9815338",
   "metadata": {},
   "outputs": [],
   "source": [
    "## google\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import httpx\n",
    "\n",
    "gclient = genai.Client()\n",
    "\n",
    "# Retrieve and encode the PDF byte\n",
    "doc_data = httpx.get(url).content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "89869765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Semantic Faithfulness and Entropy Production Measures to Tame Your LLM Demons and Manage Hallucinations\n",
      "\n",
      "## Abstract\n",
      "Evaluating faithfulness of Large Language Models (LLMs) to a given task is a complex challenge. We propose two new unsupervised metrics for faithfulness evaluation using insights from information theory and thermodynamics. Our approach treats an LLM as a bipartite information engine where hidden layers act as a Maxwell demon controlling transformations of context C into answer A via prompt Q. We model Question-Context-Answer (QCA) triplets as probability distributions over shared topics. Topic transformations from C to Q and A are modeled as transition matrices Q and A encoding the query goal and actual result, respectively. Our semantic faithfulness (SF) metric quantifies faithfulness for any given QCA triplet by the Kullback-Leibler (KL) divergence between these matrices. Both matrices are inferred simultaneously via convex optimization of this KL divergence, and the final SF metric is obtained by mapping the minimal divergence onto the unit interval [0,1], where higher scores indicate greater faithfulness. Furthermore, we propose a thermodynamics-based semantic entropy production (SEP) metric in answer generation, and show that high faithfulness generally implies low entropy production. The SF and SEP metrics can be used jointly or separately for LLM evaluation and hallucination control. We demonstrate our framework on LLM summarization of corporate SEC 10-K filings.\n",
      "\n",
      "## Summary\n",
      "\n",
      "*   **Problem:** Evaluating LLM faithfulness and controlling hallucinations is a critical, complex challenge, often relying on expensive human annotation or biased LLM-as-a-Judge methods.\n",
      "*   **Solution:** Introduction of two novel, unsupervised, quantitative, and interpretable metrics:\n",
      "    *   **Semantic Faithfulness (SF):** Quantifies how well an LLM's answer semantically aligns with the question's intent regarding a given context.\n",
      "    *   **Semantic Entropy Production (SEP):** Measures the thermodynamic irreversibility (or \"heat produced\") during the LLM's answer generation process.\n",
      "*   **Methodology:**\n",
      "    *   LLM is modeled as a bipartite information engine (Maxwell's demon analogy).\n",
      "    *   QCA (Question, Context, Answer) triplets are represented as topic distributions.\n",
      "    *   Information flows are captured by topic transition matrices: **Q** (context to question) and **A** (context to answer).\n",
      "    *   SF is derived from minimizing the Kullback-Leibler (KL) divergence between **A** and **Q**, indicating semantic similarity.\n",
      "    *   SEP is derived from stochastic thermodynamics, quantifying the \"entropy produced\" by the LLM in transforming context to answer.\n",
      "*   **Key Findings:**\n",
      "    *   SF and SEP are negatively correlated (higher faithfulness implies lower entropy production), but capture distinct aspects of LLM performance.\n",
      "    *   SF is effective at detecting subtle hallucinations missed by LLM-as-a-Judge evaluations.\n",
      "*   **Applications:** LLM evaluation, hallucination detection, answer selection, reference-free evaluation, model governance, and prompt engineering.\n",
      "\n",
      "## Extended Summary\n",
      "\n",
      "The paper addresses the critical and persistent challenge of ensuring that Large Language Model (LLM) outputs are factually grounded and faithful to a provided source context, particularly concerning \"faithfulness hallucinations.\" Current evaluation methods are often costly (human annotators) or potentially biased (LLM-as-a-Judge). This work introduces a novel automated framework based on information theory and thermodynamics to provide quantitative and interpretable metrics for LLM faithfulness.\n",
      "\n",
      "**Theoretical Framework:**\n",
      "The authors model an LLM as a \"bipartite information engine,\" akin to a Maxwell's demon. This engine comprises two stochastic sub-systems: an observable X-system (representing the LLM's input context C and output answer A) and an unobserved Y-system (representing the LLM's internal computational engine and control, informed by the user's question Q).\n",
      "All components of a QCA (Question, Context, Answer) triplet are first transformed into probability distributions over a shared latent topic space of `N` topics. This is achieved using semantic embedding models (e.g., Qwen3-Embedding-0.6B) and a joint clustering method called Semantic Divergence Metrics (SDM), which also determines the optimal number of topics.\n",
      "The transformations of topics from the context (C) to the question (Q) and to the answer (A) are then represented by `N x N` row-stochastic transition matrices, **Q** (context to question) and **A** (context to answer), respectively. These matrices must satisfy constraints imposed by the observed marginal topic distributions of C, Q, and A.\n",
      "\n",
      "**Semantic Faithfulness (SF) Metric:**\n",
      "The core idea for SF is that a \"faithful\" answer should transform the topics from the context in a way that is semantically similar to how the question \"queries\" the topics from that same context. SF (denoted `Fs`) is defined as `1 / (1 + Dmin)`, where `Dmin` is the minimal conditional Kullback-Leibler (KL) divergence between the **A** and **Q** transition matrices. Minimizing this KL divergence effectively finds the most \"parsimonious\" explanation for the topic transformations. The optimization problem is convex and solved using an alternating minimization (AM) algorithm, specifically a Blahut-Arimoto (BA) type algorithm, which guarantees convergence to a global minimum. The `Fs` score ranges from 0 to 1, with higher values indicating greater faithfulness.\n",
      "\n",
      "**Semantic Entropy Production (SEP) Metric:**\n",
      "Building on stochastic thermodynamics, the paper introduces Semantic Entropy Production (SEP) as a measure of thermodynamic irreversibility during LLM answer generation. The total entropy production (`Stot`) is decomposed into two parts: the system entropy change `S = H[p(a)] - H[p(c)]` (difference in marginal entropies of answer and context) and the dissipated heat `Sm`. `Sm` represents heat exchanged with the environment and LLM's internal knowledge base. While `S` can be directly calculated, `Sm` requires estimating a reverse transition matrix `AR`. The paper proposes computing a lower bound for SEP by minimizing an expression related to `Stot` with respect to all possible `AR` matrices, again solvable via convex optimization. Intuitively, LLM hallucinations are noisy, entropy-increasing distortions, suggesting that high faithfulness should correlate with low entropy production.\n",
      "\n",
      "**Experimental Validation:**\n",
      "The framework was validated on 10 Question-Context-Answer (QCA) triplets derived from NVIDIA's fiscal 2024 10-K Risk Factors. These triplets were divided into two groups: Group A (comprehensive multi-topic risk analysis) and Group B (focused competitive threats analysis), designed to test different question semantic structures. LLM answers were generated using Gemini-2.5-Pro, while Claude Sonnet 4.5 was used for LLM-as-a-Judge evaluation.\n",
      "\n",
      "**Key Experimental Observations:**\n",
      "*   **Question Entropy Variation:** The question set exhibited meaningful entropy variation, with Group A showing broader variation (CV ≈ 25.5%) than Group B (CV ≈ 2.4%).\n",
      "*   **Faithfulness Scores:** Both groups had similar mean SF scores (`Fs` ≈ 0.51), suggesting that question type alone does not strongly determine faithfulness. However, individual triplet variations showed that specific question-context pairings are important. A positive correlation between question entropy `H(Q)` and `Fs` was observed (`r = 0.695`).\n",
      "*   **System Entropy Change:** The system entropy change `S = H(A) - H(C)` was consistently positive across all triplets (mean = 0.550 bits), indicating that LLMs generally increase semantic uncertainty when generating answers. Comprehensive questions (Group A) elicited stronger entropy expansion.\n",
      "*   **Semantic Entropy Production:** SEP values showed meaningful variation (mean = 0.287 bits), with Group A exhibiting higher SEP than Group B. The analysis revealed that for most triplets, the dissipated heat `Sm` was negative, implying the LLM draws on its internal knowledge base to offset entropy production during answer generation.\n",
      "*   **SF and SEP Relationship:** A moderate negative correlation (`r = -0.612`) was found between SF and SEP, supporting the hypothesis that higher faithfulness corresponds to lower entropy production. This confirms they are related but distinct metrics.\n",
      "*   **LLM-as-a-Judge Comparison:** In a qualitative evaluation, SF correctly assigned a lower faithfulness score (0.250) to an answer containing a subtle hallucination (fabricated customer names) that an LLM judge (Claude Sonnet 4.5) failed to recognize, even rating it highly for coherence and relevance. This highlights SF's ability to detect semantic deviations missed by surface-level LLM evaluations.\n",
      "\n",
      "**Conclusions and Practical Applications:**\n",
      "The paper concludes that SF and SEP are complementary metrics. SF quantifies information-theoretic alignment, while SEP measures the thermodynamic cost of context-to-answer transformation. Both should be used together for comprehensive LLM evaluation. Practical applications include answer selection/ranking, early detection of hallucinations, reference-free evaluation, LLM model governance for high-stakes domains, and actionable insights for prompt engineering.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"You are a concise document summariser. Read the PDF at the provided URL directly and\n",
    "            return a clear, structured summary with key points, important details, and conclusions.\n",
    "            Present your output in four sections: 'Title', 'Abstract', 'Summary' and 'Extended Summary'.\n",
    "\n",
    "            In 'Title', extract the document title.\n",
    "            In 'Abstract', extract the document abstract.\n",
    "            Into 'Summary', include only the most critical information in brief bullet points.\n",
    "            In 'Extended Summary', provide a more detailed explanation with relevant context.\n",
    "            Format your response using markdown with appropriate headings and bullet points.\n",
    "            \"\"\"\n",
    "response = gclient.models.generate_content(\n",
    "  model=\"gemini-2.5-flash\",\n",
    "  contents=[\n",
    "      types.Part.from_bytes(\n",
    "        data=doc_data,\n",
    "        mime_type='application/pdf',\n",
    "      ),\n",
    "      prompt])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a4472c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Title: Semantic Faithfulness and Entropy Production Measures to Tame Your LLM Demons and Manage Hallucinations\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Extract title from the first # heading\n",
    "lines = response.text.split('\\n')\n",
    "title = None\n",
    "\n",
    "for line in lines:\n",
    "    if line.strip().startswith('#'):\n",
    "        # Remove the # symbols and strip whitespace\n",
    "        title = re.sub(r'^#+\\s*', '', line).strip()\n",
    "        break\n",
    "\n",
    "print(f\"Extracted Title: {title}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d912e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Extract JSON from markdown code blocks if present\n",
    "text = response.text\n",
    "match = re.search(r'```(?:json)?\\s*([\\s\\S]*?)\\s*```', text)\n",
    "if match:\n",
    "    json_text = match.group(1)\n",
    "else:\n",
    "    json_text = text\n",
    "\n",
    "# Parse to JSON\n",
    "json_output = json.loads(json_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e98e465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The paper introduces the Kamiwaza Agentic Merit Index (KAMI) v0.1, a new benchmark designed to address the shortcomings of traditional LLM evaluations for enterprise-relevant agentic AI. Existing benchmarks suffer from issues like training data contamination and a failure to measure true agentic capabilities such as multi-step tool use, decision-making under uncertainty, and real-world task completion. KAMI v0.1 leverages the PICARD framework to ensure contamination resistance and realistic task environments, focusing on common enterprise use cases like filesystem operations, text search/extraction, CSV processing, and database querying.\\n\\nThe evaluation involved 35 unique model configurations across AMD MI300X, Intel Gaudi 3, and Anthropic's API, processing over 5.5 billion tokens across 170,000 test conversations. Key findings reveal a 'persistent agentic disconnect': models that perform well on academic benchmarks often underperform on KAMI's practical enterprise tasks, and vice versa. For instance, the original Qwen3 models (with 'hybrid thinking') surprisingly underperformed their older Qwen2.5 counterparts on KAMI, despite excelling on popular benchmarks. Similarly, models like Llama 3.1 70B and Claude 3.5 Haiku, which rank poorly on aggregated benchmarks like Artificial Analysis Intelligence Index (AAII), demonstrate strong, reliable performance in KAMI v0.1.\\n\\nThe research also provides insights into cost-performance tradeoffs, particularly regarding reasoning models. While reasoning capabilities can boost accuracy for smaller models, they incur a substantial increase (10-14x) in tokens generated and wall-time (4-6x), making them less efficient for routine agentic tasks. Furthermore, the study highlights the critical role of 'context engineering,' demonstrating that even slight hints in prompts can drastically lower task costs and improve reliability (e.g., q601 vs. q501), but conversely, poorly designed hints can degrade performance by causing unintended mental model shifts (e.g., q602 vs. q502). The messaging from tools (e.g., 'Code executed successfully with no output') was also found to significantly confuse LLMs, emphasizing that tool descriptions, parameters, and success/failure messages are crucial components of effective context engineering for robust agentic systems.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_output.get('Extended Summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "da45c4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_str_list(str_list):\n",
    "    if isinstance(str_list, list):\n",
    "        if len(str_list) > 0:\n",
    "            return \"\\n\".join(str_list)\n",
    "    elif isinstance(str_list, str):\n",
    "        return str_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cb65db59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Traditional LLM benchmarks often fail to predict real-world agentic performance due to data contamination and lack of agentic capability measurement.\\nThe Kamiwaza Agentic Merit Index (KAMI) v0.1 is introduced as an enterprise-focused benchmark, using the PICARD framework for contamination resistance and agentic evaluation.\\nEvaluations across 35 model configurations and over 5.5 billion tokens reveal a 'persistent agentic disconnect', where traditional benchmark rankings poorly predict practical agentic performance.\\nNewer models (e.g., Llama 4, Qwen 3) do not consistently outperform older variants (e.g., Qwen 2.5 72B) on enterprise-relevant tasks.\\nReasoning models significantly increase token usage and wall-time for a modest accuracy gain, suggesting poor cost-performance tradeoffs for common tasks.\\nSubtle changes in prompts and tool feedback can drastically impact LLM performance and cost, emphasizing the importance of 'context engineering'.\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_output.get(\"Summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5f531a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Towards a Standard, Enterprise-Relevant Agentic AI Benchmark: Lessons from 5.5 billion tokens' worth of agentic AI evaluations\n",
      "\n",
      "**Summary**\n",
      "Traditional LLM benchmarks often fail to predict real-world agentic performance due to data contamination and lack of agentic capability measurement.\n",
      "The Kamiwaza Agentic Merit Index (KAMI) v0.1 is introduced as an enterprise-focused benchmark, using the PICARD framework for contamination resistance and agentic evaluation.\n",
      "Evaluations across 35 model configurations and over 5.5 billion tokens reveal a 'persistent agentic disconnect', where traditional benchmark rankings poorly predict practical agentic performance.\n",
      "Newer models (e.g., Llama 4, Qwen 3) do not consistently outperform older variants (e.g., Qwen 2.5 72B) on enterprise-relevant tasks.\n",
      "Reasoning models significantly increase token usage and wall-time for a modest accuracy gain, suggesting poor cost-performance tradeoffs for common tasks.\n",
      "Subtle changes in prompts and tool feedback can drastically impact LLM performance and cost, emphasizing the importance of 'context engineering'.\n",
      "\n",
      "**Extended Summary**\n",
      "The paper introduces the Kamiwaza Agentic Merit Index (KAMI) v0.1, a new benchmark designed to address the shortcomings of traditional LLM evaluations for enterprise-relevant agentic AI. Existing benchmarks suffer from issues like training data contamination and a failure to measure true agentic capabilities such as multi-step tool use, decision-making under uncertainty, and real-world task completion. KAMI v0.1 leverages the PICARD framework to ensure contamination resistance and realistic task environments, focusing on common enterprise use cases like filesystem operations, text search/extraction, CSV processing, and database querying.\n",
      "\n",
      "The evaluation involved 35 unique model configurations across AMD MI300X, Intel Gaudi 3, and Anthropic's API, processing over 5.5 billion tokens across 170,000 test conversations. Key findings reveal a 'persistent agentic disconnect': models that perform well on academic benchmarks often underperform on KAMI's practical enterprise tasks, and vice versa. For instance, the original Qwen3 models (with 'hybrid thinking') surprisingly underperformed their older Qwen2.5 counterparts on KAMI, despite excelling on popular benchmarks. Similarly, models like Llama 3.1 70B and Claude 3.5 Haiku, which rank poorly on aggregated benchmarks like Artificial Analysis Intelligence Index (AAII), demonstrate strong, reliable performance in KAMI v0.1.\n",
      "\n",
      "The research also provides insights into cost-performance tradeoffs, particularly regarding reasoning models. While reasoning capabilities can boost accuracy for smaller models, they incur a substantial increase (10-14x) in tokens generated and wall-time (4-6x), making them less efficient for routine agentic tasks. Furthermore, the study highlights the critical role of 'context engineering,' demonstrating that even slight hints in prompts can drastically lower task costs and improve reliability (e.g., q601 vs. q501), but conversely, poorly designed hints can degrade performance by causing unintended mental model shifts (e.g., q602 vs. q502). The messaging from tools (e.g., 'Code executed successfully with no output') was also found to significantly confuse LLMs, emphasizing that tool descriptions, parameters, and success/failure messages are crucial components of effective context engineering for robust agentic systems.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary = concat_str_list(json_output.get(\"Summary\"))\n",
    "ext_summary = json_output.get('Extended Summary')\n",
    "title = json_output.get(\"Title\", \"Untitled\")  # Notion title limit is [:100]\n",
    "print(title)\n",
    "\n",
    "content = f\"\"\"\n",
    "**Summary**\n",
    "{summary}\n",
    "\n",
    "**Extended Summary**\n",
    "{ext_summary}\n",
    "\"\"\"\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f75c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "NOTION_TOKEN = os.getenv(\"NOTION_TOKEN\")\n",
    "DATABASE_ID = os.getenv(\"NOTION_DATABASE_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f55a7982",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notion_client import Client\n",
    "notion = Client(auth=os.environ[\"NOTION_TOKEN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cd77491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from notion_client import Client\n",
    "\n",
    "\n",
    "def chunk_content(content, chunk_size=2000):\n",
    "    \"\"\"\n",
    "    Split content into chunks of max chunk_size characters, breaking at word boundaries.\n",
    "\n",
    "    Args:\n",
    "        content: String content to chunk\n",
    "        chunk_size: Maximum characters per chunk (default 2000)\n",
    "\n",
    "    Returns:\n",
    "        List of content chunks\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    while len(content) > 0:\n",
    "        if len(content) <= chunk_size:\n",
    "            chunks.append(content)\n",
    "            break\n",
    "\n",
    "        # Find the last space before chunk_size to avoid breaking words\n",
    "        chunk_end = chunk_size\n",
    "        last_space = content.rfind(' ', 0, chunk_size)\n",
    "\n",
    "        if last_space > 0:\n",
    "            chunk_end = last_space\n",
    "\n",
    "        chunks.append(content[:chunk_end].strip())\n",
    "        content = content[chunk_end:].strip()\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def add_content_to_page(notion_token, page_id, content):\n",
    "    \"\"\"\n",
    "    Add chunked content to a Notion page as paragraph blocks.\n",
    "\n",
    "    Args:\n",
    "        notion_token: Notion API token\n",
    "        page_id: ID of the page to add content to\n",
    "        content: String content to add\n",
    "    \"\"\"\n",
    "    notion = Client(auth=notion_token)\n",
    "    chunks = chunk_content(content, chunk_size=2000)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        notion.blocks.children.append(\n",
    "            block_id=page_id,\n",
    "            children=[\n",
    "                {\n",
    "                    \"object\": \"block\",\n",
    "                    \"type\": \"paragraph\",\n",
    "                    \"paragraph\": {\n",
    "                        \"rich_text\": [\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                \"text\": {\n",
    "                                    \"content\": chunk\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "def write_to_notion(title, url, content, notion_token, database_id):\n",
    "    \"\"\"\n",
    "    Write JSON output (Summary, Extended Summary) to a Notion database page.\n",
    "\n",
    "    Args:\n",
    "        title: Document title\n",
    "        url: Document URL\n",
    "        content: Content to add to the page (will be chunked if needed)\n",
    "        notion_token: Your Notion API token\n",
    "        database_id: The ID of your 'Technical Resources 2025-2026' database\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the Notion client\n",
    "    notion = Client(auth=notion_token)\n",
    "\n",
    "    # Create a new page in the database\n",
    "    new_page = notion.pages.create(\n",
    "        parent={\"database_id\": database_id},\n",
    "        properties={\n",
    "            \"Title\": {\n",
    "                \"title\": [\n",
    "                    {\n",
    "                        \"text\": {\n",
    "                            \"content\": title\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \"URL\": {\n",
    "                \"url\": url\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Add content to the page if provided\n",
    "    if content:\n",
    "        add_content_to_page(notion_token, new_page['id'], content)\n",
    "\n",
    "    return new_page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090129ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page created successfully: 2e2be5eb-a41a-81e6-86e1-f0ae2803c8ea\n"
     ]
    }
   ],
   "source": [
    "result = write_to_notion(title, url, content, NOTION_TOKEN, DATABASE_ID)\n",
    "print(f\"Page created successfully: {result['id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "dd117f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_content_result = add_content_to_page(NOTION_TOKEN, result['id'], response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "16e1c127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The paper introduces the Kamiwaza Agentic Merit Index (KAMI) v0.1, a new benchmark designed to address the shortcomings of traditional LLM evaluations for enterprise-relevant agentic AI. Existing benchmarks suffer from issues like training data contamination and a failure to measure true agentic capabilities such as multi-step tool use, decision-making under uncertainty, and real-world task completion. KAMI v0.1 leverages the PICARD framework to ensure contamination resistance and realistic task environments, focusing on common enterprise use cases like filesystem operations, text search/extraction, CSV processing, and database querying.\\n\\nThe evaluation involved 35 unique model configurations across AMD MI300X, Intel Gaudi 3, and Anthropic's API, processing over 5.5 billion tokens across 170,000 test conversations. Key findings reveal a 'persistent agentic disconnect': models that perform well on academic benchmarks often underperform on KAMI's practical enterprise tasks, and vice versa. For instance, the original Qwen3 models (with 'hybrid thinking') surprisingly underperformed their older Qwen2.5 counterparts on KAMI, despite excelling on popular benchmarks. Similarly, models like Llama 3.1 70B and Claude 3.5 Haiku, which rank poorly on aggregated benchmarks like Artificial Analysis Intelligence Index (AAII), demonstrate strong, reliable performance in KAMI v0.1.\\n\\nThe research also provides insights into cost-performance tradeoffs, particularly regarding reasoning models. While reasoning capabilities can boost accuracy for smaller models, they incur a substantial increase (10-14x) in tokens generated and wall-time (4-6x), making them less efficient for routine agentic tasks. Furthermore, the study highlights the critical role of 'context engineering,' demonstrating that even slight hints in prompts can drastically lower task costs and improve reliability (e.g., q601 vs. q501), but conversely, poorly designed hints can degrade performance by causing unintended mental model shifts (e.g., q602 vs. q502). The messaging from tools (e.g., 'Code executed successfully with no output') was also found to significantly confuse LLMs, emphasizing that tool descriptions, parameters, and success/failure messages are crucial components of effective context engineering for robust agentic systems.\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_output.get(\"Extended Summary\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8497ee56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Gemini PDF Summary\n",
       "\n",
       "This paper, \"The Illusion of Thinking,\" investigates the fundamental strengths and limitations of Large Reasoning Models (LRMs) like Claude 3.7 Sonnet Thinking and DeepSeek-R1, particularly how their reasoning capabilities scale with problem complexity.\n",
       "\n",
       "**Key Points:**\n",
       "\n",
       "1.  **Controllable Evaluation:** The authors introduce a novel evaluation paradigm using four controllable puzzle environments (Tower of Hanoi, Checker Jumping, River Crossing, Blocks World). These allow precise manipulation of compositional complexity and detailed analysis of internal reasoning traces, circumventing data contamination issues common in standard benchmarks.\n",
       "2.  **Accuracy Collapse:** State-of-the-art LRMs exhibit a complete accuracy collapse beyond certain complexity thresholds across all puzzle environments, indicating a lack of generalizable problem-solving capabilities.\n",
       "3.  **Counter-intuitive Scaling Limit in Reasoning Effort:** LRMs' reasoning effort (measured by inference tokens) initially increases with problem complexity but then *declines* sharply as problems approach the critical collapse point, despite having ample token budget. This suggests a fundamental scaling limitation.\n",
       "4.  **Three Reasoning Regimes:**\n",
       "    *   **Low Complexity:** Standard (non-thinking) LLMs often surprisingly outperform LRMs and are more token-efficient.\n",
       "    *   **Medium Complexity:** LRMs demonstrate an advantage due to their detailed thinking processes.\n",
       "    *   **High Complexity:** Both thinking and non-thinking models experience complete performance collapse.\n",
       "\n",
       "**Important Details:**\n",
       "\n",
       "*   **Analysis of Reasoning Traces:**\n",
       "    *   **\"Overthinking\" (Low Complexity):** For simpler problems, LRMs often identify correct solutions early but then inefficiently continue exploring incorrect alternatives.\n",
       "    *   **Moderate Complexity:** Correct solutions emerge later in the thinking process, often after extensive exploration of incorrect paths.\n",
       "    *   **High Complexity:** Models consistently fail to find any correct solutions.\n",
       "*   **Limitations in Exact Computation:**\n",
       "    *   Providing explicit algorithms (e.g., for Tower of Hanoi) in the prompt does not significantly improve LRM performance, suggesting limitations in consistent logical step execution and verification, not just solution discovery.\n",
       "    *   Models show inconsistent reasoning across puzzle types; for instance, Claude 3.7 Sonnet performs well on Tower of Hanoi (N=5, 31 moves) but fails early on River Crossing (N=3, 11 moves), implying sensitivity to training data distribution.\n",
       "\n",
       "**Conclusion:**\n",
       "\n",
       "The study concludes that despite sophisticated self-reflection mechanisms, current LRMs possess fundamental limitations in generalizable reasoning, exhibit counter-intuitive scaling behaviors, and struggle with exact computation and consistent logical execution. These findings challenge prevailing assumptions about LRM capabilities and underscore the need for new approaches and evaluation paradigms to advance towards more robust artificial intelligence."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"## Gemini PDF Summary\\n\\n{response.text}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5c786a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'object': 'block', 'type': 'heading_1', 'heading_1': {'rich_text': [{'type': 'text', 'text': {'content': 'Semantic Faithfulness and Entropy Production Measures to Tame Your LLM Demons and Manage Hallucinations'}}]}}, {'object': 'block', 'type': 'heading_2', 'heading_2': {'rich_text': [{'type': 'text', 'text': {'content': 'Abstract'}}]}}, {'object': 'block', 'type': 'paragraph', 'paragraph': {'rich_text': [{'type': 'text', 'text': {'content': 'Evaluating faithfulness of Large Language Models (LLMs) to a given task is a complex challenge. We propose two new unsupervised metrics for faithfulness evaluation using insights from information theory and thermodynamics. Our approach treats an LLM as a bipartite information engine where hidden layers act as a Maxwell demon controlling transformations of context C into answer A via prompt Q. We model Question-Context-Answer (QCA) triplets as probability distributions over shared topics. Topic transformations from C to Q and A are modeled as transition matrices Q and A encoding the query goal and actual result, respectively. Our semantic faithfulness (SF) metric quantifies faithfulness for any given QCA triplet by the Kullback-Leibler (KL) divergence between these matrices. Both matrices are inferred simultaneously via convex optimization of this KL divergence, and the final SF metric is obtained by mapping the minimal divergence onto the unit interval [0,1], where higher scores indicate greater faithfulness. Furthermore, we propose a thermodynamics-based semantic entropy production (SEP) metric in answer generation, and show that high faithfulness generally implies low entropy production. The SF and SEP metrics can be used jointly or separately for LLM evaluation and hallucination control. We demonstrate our framework on LLM summarization of corporate SEC 10-K filings.'}}]}}, {'object': 'block', 'type': 'heading_2', 'heading_2': {'rich_text': [{'type': 'text', 'text': {'content': 'Summary'}}]}}, {'object': 'block', 'type': 'paragraph', 'paragraph': {'rich_text': [{'type': 'text', 'text': {'content': '   '}, 'annotations': {'italic': True}}, {'type': 'text', 'text': {'content': 'Problem:'}, 'annotations': {'italic': True}}, {'type': 'text', 'text': {'content': '* Evaluating LLM faithfulness and controlling hallucinations is a critical, complex challenge, often relying on expensive human annotation or biased LLM-as-a-Judge methods.'}}]}}, {'object': 'block', 'type': 'paragraph', 'paragraph': {'rich_text': [{'type': 'text', 'text': {'content': '   '}, 'annotations': {'italic': True}}, {'type': 'text', 'text': {'content': 'Solution:'}, 'annotations': {'italic': True}}, {'type': 'text', 'text': {'content': '* Introduction of two novel, unsupervised, quantitative, and interpretable metrics:'}}]}}, {'object': 'block', 'type': 'paragraph', 'paragraph': {'rich_text': [{'type': 'text', 'text': {'content': '   '}, 'annotations': {'italic': True}}, {'type': 'text', 'text': {'content': 'Semantic Faithfulness (SF):'}, 'annotations': {'italic': True}}, {'type': 'text', 'text': {'content': \"* Quantifies how well an LLM's answer semantically aligns with the question's intent regarding a given context.\"}}]}}, {'object': 'block', 'type': 'paragraph', 'paragraph': {'rich_text': [{'type': 'text', 'text': {'content': '   '}, 'annotations': {'italic': True}}, {'type': 'text', 'text': {'content': 'Semantic Entropy Production (SEP):'}, 'annotations': {'italic': True}}, {'type': 'text', 'text': {'content': '* Measures the thermodynamic irreversibility (or \"heat produced\") during the LLM\\'s answer generation process.'}}]}}, {'object': 'block', 'type': 'paragraph', 'paragraph': {'rich_text': [{'type': 'text', 'text': {'content': '   '}, 'annotations': {'italic': True}}, {'type': 'text', 'text': {'content': 'Methodology:'}, 'annotations': {'italic': True}}, {'type': 'text', 'text': {'content': '*'}}]}}, {'object': 'block', 'type': 'paragraph', 'paragraph': {'rich_text': [{'type': 'text', 'text': {'content': \"*   LLM is modeled as a bipartite information engine (Maxwell's demon analogy).\"}}]}}, {'object': 'block', 'type': 'paragraph', 'paragraph': {'rich_text': [{'type': 'text', 'text': {'content': '*   QCA (Question, Context, Answer) triplets are represented as topic distributions.'}}]}}, {'object': 'block', 'type': 'paragraph', 'paragraph': {'rich_text': [{'type': 'text', 'text': {'content': '   Information flows are captured by topic transition matrices: '}, 'annotations': {'italic': True}}, {'type': 'text', 'text': {'content': 'Q'}, 'annotations': {'italic': True}}, {'type': 'text', 'text': {'content': ' (context to question) and '}, 'annotations': {'italic': True}}, {'type': 'text', 'text': {'content': 'A'}, 'annotations': {'italic': True}}, {'type': 'text', 'text': {'content': '* (context to answer).'}}]}}, {'object': 'block', 'type': 'paragraph', 'paragraph': {'rich_text': [{'type': 'text', 'text': {'content': '   SF is derived from minimizing the Kullback-Leibler (KL) divergence between '}, 'annotations': {'italic': True}}, {'type': 'text', 'text': {'content': 'A'}, 'annotations': {'italic': True}}, {'type': 'text', 'text': {'content': ' and '}, 'annotations': {'italic': True}}, {'type': 'text', 'text': {'content': 'Q'}, 'annotations': {'italic': True}}, {'type': 'text', 'text': {'content': '*, indicating semantic similarity.'}}]}}, {'object': 'block', 'type': 'paragraph', 'paragraph': {'rich_text': [{'type': 'text', 'text': {'content': '*   SEP is derived from stochastic thermodynamics, quantifying the \"entropy produced\" by the LLM in transforming context to answer.'}}]}}, {'object': 'block', 'type': 'paragraph', 'paragraph': {'rich_text': [{'type': 'text', 'text': {'content': '   '}, 'annotations': {'italic': True}}, {'type': 'text', 'text': {'content': 'Key Findings:'}, 'annotations': {'italic': True}}, {'type': 'text', 'text': {'content': '*'}}]}}, {'object': 'block', 'type': 'paragraph', 'paragraph': {'rich_text': [{'type': 'text', 'text': {'content': '*   SF and SEP are negatively correlated (higher faithfulness implies lower entropy production), but capture distinct aspects of LLM performance.'}}]}}, {'object': 'block', 'type': 'paragraph', 'paragraph': {'rich_text': [{'type': 'text', 'text': {'content': '*   SF is effective at detecting subtle hallucinations missed by LLM-as-a-Judge evaluations.'}}]}}, {'object': 'block', 'type': 'paragraph', 'paragraph': {'rich_text': [{'type': 'text', 'text': {'content': '   '}, 'annotations': {'italic': True}}, {'type': 'text', 'text': {'content': 'Applications:'}, 'annotations': {'italic': True}}, {'type': 'text', 'text': {'content': '* LLM evaluation, hallucination detection, answer selection, reference-free evaluation, model governance, and prompt engineering.'}}]}}, {'object': 'block', 'type': 'heading_2', 'heading_2': {'rich_text': [{'type': 'text', 'text': {'content': 'Extended Summary'}}]}}, {'object': 'block', 'type': 'paragraph', 'paragraph': {'rich_text': [{'type': 'text', 'text': {'content': 'The paper addresses the critical and persistent challenge of ensuring that Large Language Model (LLM) outputs are factually grounded and faithful to a provided source context, particularly concerning \"faithfulness hallucinations.\" Current evaluation methods are often costly (human annotators) or potentially biased (LLM-as-a-Judge). This work introduces a novel automated framework based on information theory and thermodynamics to provide quantitative and interpretable metrics for LLM faithfulness.'}}]}}, {'object': 'block', 'type': 'paragraph', 'paragraph': {'rich_text': [{'type': 'text', 'text': {'content': 'Theoretical Framework:'}, 'annotations': {'bold': True}}]}}, {'object': 'block', 'type': 'paragraph', 'paragraph': {'rich_text': [{'type': 'text', 'text': {'content': 'The authors model an LLM as a \"bipartite information engine,\" akin to a Maxwell\\'s demon. This engine comprises two stochastic sub-systems: an observable X-system (representing the LLM\\'s input context C and output answer A) and an unobserved Y-system (representing the LLM\\'s internal computational engine and control, informed by the user\\'s question Q).'}}]}}, {'object': 'block', 'type': 'paragraph', 'paragraph': {'rich_text': [{'type': 'text', 'text': {'content': 'All components of a QCA (Question, Context, Answer) triplet are first transformed into probability distributions over a shared latent topic space of `N` topics. This is achieved using semantic embedding models (e.g., Qwen3-Embedding-0.6B) and a joint clustering method called Semantic Divergence Metrics (SDM), which also determines the optimal number of topics.'}}]}}, {'object': 'block', 'type': 'paragraph', 'paragraph': {'rich_text': [{'type': 'text', 'text': {'content': 'The transformations of topics from the context (C) to the question (Q) and to the answer (A) are then represented by `N x N` row-stochastic transition matrices, '}}, {'type': 'text', 'text': {'content': 'Q'}, 'annotations': {'bold': True}}, {'type': 'text', 'text': {'content': ' (context to question) and '}}, {'type': 'text', 'text': {'content': 'A'}, 'annotations': {'bold': True}}, {'type': 'text', 'text': {'content': ' (context to answer), respectively. These matrices must satisfy constraints imposed by the observed marginal topic distributions of C, Q, and A.'}}]}}, {'object': 'block', 'type': 'paragraph', 'paragraph': {'rich_text': [{'type': 'text', 'text': {'content': 'Semantic Faithfulness (SF) Metric:'}, 'annotations': {'bold': True}}]}}, {'object': 'block', 'type': 'paragraph', 'paragraph': {'rich_text': [{'type': 'text', 'text': {'content': 'The core idea for SF is that a \"faithful\" answer should transform the topics from the context in a way that is semantically similar to how the question \"queries\" the topics from that same context. SF (denoted `Fs`) is defined as `1 / (1 + Dmin)`, where `Dmin` is the minimal conditional Kullback-Leibler (KL) divergence between the '}}, {'type': 'text', 'text': {'content': 'A'}, 'annotations': {'bold': True}}, {'type': 'text', 'text': {'content': ' and '}}, {'type': 'text', 'text': {'content': 'Q'}, 'annotations': {'bold': True}}, {'type': 'text', 'text': {'content': ' transition matrices. Minimizing this KL divergence effectively finds the most \"parsimonious\" explanation for the topic transformations. The optimization problem is convex and solved using an alternating minimization (AM) algorithm, specifically a Blahut-Arimoto (BA) type algorithm, which guarantees convergence to a global minimum. The `Fs` score ranges from 0 to 1, with higher values indicating greater faithfulness.'}}]}}, {'object': 'block', 'type': 'paragraph', 'paragraph': {'rich_text': [{'type': 'text', 'text': {'content': 'Semantic Entropy Production (SEP) Metric:'}, 'annotations': {'bold': True}}]}}, {'object': 'block', 'type': 'paragraph', 'paragraph': {'rich_text': [{'type': 'text', 'text': {'content': \"Building on stochastic thermodynamics, the paper introduces Semantic Entropy Production (SEP) as a measure of thermodynamic irreversibility during LLM answer generation. The total entropy production (`Stot`) is decomposed into two parts: the system entropy change `S = H[p(a)] - H[p(c)]` (difference in marginal entropies of answer and context) and the dissipated heat `Sm`. `Sm` represents heat exchanged with the environment and LLM's internal knowledge base. While `S` can be directly calculated, `Sm` requires estimating a reverse transition matrix `AR`. The paper proposes computing a lower bound for SEP by minimizing an expression related to `Stot` with respect to all possible `AR` matrices, again solvable via convex optimization. Intuitively, LLM hallucinations are noisy, entropy-increasing distortions, suggesting that high faithfulness should correlate with low entropy production.\"}}]}}, {'object': 'block', 'type': 'paragraph', 'paragraph': {'rich_text': [{'type': 'text', 'text': {'content': 'Experimental Validation:'}, 'annotations': {'bold': True}}]}}, {'object': 'block', 'type': 'paragraph', 'paragraph': {'rich_text': [{'type': 'text', 'text': {'content': \"The framework was validated on 10 Question-Context-Answer (QCA) triplets derived from NVIDIA's fiscal 2024 10-K Risk Factors. These triplets were divided into two groups: Group A (comprehensive multi-topic risk analysis) and Group B (focused competitive threats analysis), designed to test different question semantic structures. LLM answers were generated using Gemini-2.5-Pro, while Claude Sonnet 4.5 was used for LLM-as-a-Judge evaluation.\"}}]}}, {'object': 'block', 'type': 'paragraph', 'paragraph': {'rich_text': [{'type': 'text', 'text': {'content': 'Key Experimental Observations:'}, 'annotations': {'bold': True}}]}}, {'object': 'block', 'type': 'paragraph', 'paragraph': {'rich_text': [{'type': 'text', 'text': {'content': '   '}, 'annotations': {'italic': True}}, {'type': 'text', 'text': {'content': 'Question Entropy Variation:'}, 'annotations': {'italic': True}}, {'type': 'text', 'text': {'content': '* The question set exhibited meaningful entropy variation, with Group A showing broader variation (CV ≈ 25.5%) than Group B (CV ≈ 2.4%).'}}]}}, {'object': 'block', 'type': 'paragraph', 'paragraph': {'rich_text': [{'type': 'text', 'text': {'content': '   '}, 'annotations': {'italic': True}}, {'type': 'text', 'text': {'content': 'Faithfulness Scores:'}, 'annotations': {'italic': True}}, {'type': 'text', 'text': {'content': '* Both groups had similar mean SF scores (`Fs` ≈ 0.51), suggesting that question type alone does not strongly determine faithfulness. However, individual triplet variations showed that specific question-context pairings are important. A positive correlation between question entropy `H(Q)` and `Fs` was observed (`r = 0.695`).'}}]}}, {'object': 'block', 'type': 'paragraph', 'paragraph': {'rich_text': [{'type': 'text', 'text': {'content': '   '}, 'annotations': {'italic': True}}, {'type': 'text', 'text': {'content': 'System Entropy Change:'}, 'annotations': {'italic': True}}, {'type': 'text', 'text': {'content': '* The system entropy change `S = H(A) - H(C)` was consistently positive across all triplets (mean = 0.550 bits), indicating that LLMs generally increase semantic uncertainty when generating answers. Comprehensive questions (Group A) elicited stronger entropy expansion.'}}]}}, {'object': 'block', 'type': 'paragraph', 'paragraph': {'rich_text': [{'type': 'text', 'text': {'content': '   '}, 'annotations': {'italic': True}}, {'type': 'text', 'text': {'content': 'Semantic Entropy Production:'}, 'annotations': {'italic': True}}, {'type': 'text', 'text': {'content': '* SEP values showed meaningful variation (mean = 0.287 bits), with Group A exhibiting higher SEP than Group B. The analysis revealed that for most triplets, the dissipated heat `Sm` was negative, implying the LLM draws on its internal knowledge base to offset entropy production during answer generation.'}}]}}, {'object': 'block', 'type': 'paragraph', 'paragraph': {'rich_text': [{'type': 'text', 'text': {'content': '   '}, 'annotations': {'italic': True}}, {'type': 'text', 'text': {'content': 'SF and SEP Relationship:'}, 'annotations': {'italic': True}}, {'type': 'text', 'text': {'content': '* A moderate negative correlation (`r = -0.612`) was found between SF and SEP, supporting the hypothesis that higher faithfulness corresponds to lower entropy production. This confirms they are related but distinct metrics.'}}]}}, {'object': 'block', 'type': 'paragraph', 'paragraph': {'rich_text': [{'type': 'text', 'text': {'content': '   '}, 'annotations': {'italic': True}}, {'type': 'text', 'text': {'content': 'LLM-as-a-Judge Comparison:'}, 'annotations': {'italic': True}}, {'type': 'text', 'text': {'content': \"* In a qualitative evaluation, SF correctly assigned a lower faithfulness score (0.250) to an answer containing a subtle hallucination (fabricated customer names) that an LLM judge (Claude Sonnet 4.5) failed to recognize, even rating it highly for coherence and relevance. This highlights SF's ability to detect semantic deviations missed by surface-level LLM evaluations.\"}}]}}, {'object': 'block', 'type': 'paragraph', 'paragraph': {'rich_text': [{'type': 'text', 'text': {'content': 'Conclusions and Practical Applications:'}, 'annotations': {'bold': True}}]}}, {'object': 'block', 'type': 'paragraph', 'paragraph': {'rich_text': [{'type': 'text', 'text': {'content': 'The paper concludes that SF and SEP are complementary metrics. SF quantifies information-theoretic alignment, while SEP measures the thermodynamic cost of context-to-answer transformation. Both should be used together for comprehensive LLM evaluation. Practical applications include answer selection/ranking, early detection of hallucinations, reference-free evaluation, LLM model governance for high-stakes domains, and actionable insights for prompt engineering.'}}]}}]\n"
     ]
    }
   ],
   "source": [
    "from utils import markdown_to_notion_blocks\n",
    "blocks = markdown_to_notion_blocks(response.text)\n",
    "print(blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "30971c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2e2be5eb-a41a-81e6-86e1-f0ae2803c8ea'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9c8d0720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import add_content_to_page\n",
    "add_content_to_page(NOTION_TOKEN, result['id'], response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
