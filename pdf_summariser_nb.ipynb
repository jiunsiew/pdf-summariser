{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd71ba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_summariser import summarise_url\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "url=\"https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f75750b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT\n",
    "model='gpt-4.1-nano'\n",
    "client = OpenAI(api_key=api_key)\n",
    "pdf_summary = summarise_url(client, url, model=model)\n",
    "print(pdf_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefc513e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db48ef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_summary = {'response_id': 'resp_0fc9878f2d45213200693ba97cfd288193a873b26c0e7af002',\n",
    " 'model': 'gpt-4.1-nano-2025-04-14',\n",
    " 'summary': 'The document titled \"The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity\" presents a comprehensive analysis of Large Reasoning Models (LRMs) and their reasoning capabilities across various puzzle environments. The key points, important details, and conclusions are summarized below:\\n\\n### Key Points:\\n- **Objective:** To systematically investigate the reasoning capabilities and limitations of frontier LRMs using controllable puzzle environments that allow precise manipulation of problem complexity.\\n- **Methodology:** Use of four puzzle environments (Tower of Hanoi, Checker Jumping, River Crossing, Blocks World) to evaluate models\\' reasoning processes, internal traces, and performance across different complexity levels.\\n- **Models Analyzed:** Several models including Claude-3.7-Sonnet (thinking/non-thinking), DeepSeek-R1/V3, and o3-mini, with access to reasoning traces.\\n- **Evaluation Approach:** Focus on both final answer accuracy and internal reasoning traces, including solution correctness, reasoning effort, and failure analysis.\\n\\n### Important Details:\\n- **Findings on Performance:**\\n  - LRMs fail to develop generalizable reasoning beyond certain complexity thresholds, with performance collapsing to zero at high complexity.\\n  - A counterintuitive scaling limit was observed: reasoning effort (tokens used for thinking) initially increases with complexity but then decreases despite increasing problem difficulty.\\n  - Three reasoning regimes identified:\\n    1. Low complexity: Non-thinking models outperform reasoning models.\\n    2. Medium complexity: Reasoning models show an advantage.\\n    3. High complexity: Both models collapse.\\n- **Analysis of Reasoning Traces:**\\n  - Models often overthink in simple problems, exploring incorrect solutions early.\\n  - In moderate problems, correct solutions emerge later, indicating extensive exploration.\\n  - In complex problems, models fail to find correct solutions, often fixating on early wrong answers.\\n- **Limitations in Exact Computation:**\\n  - LRMs struggle with explicit algorithms and exhibit inconsistent reasoning across puzzles.\\n  - Providing explicit algorithms (e.g., Tower of Hanoi solution) does not significantly improve performance.\\n- **Behavioral Insights:**\\n  - Models show non-monotonic failure patterns, with errors occurring at different points in the solution sequence depending on problem size.\\n  - Failure move distributions suggest models are more unstable and prone to inconsistent reasoning at higher complexities.\\n- **Scaling and Complexity:**\\n  - Compositional depth (number of moves) scales exponentially or quadratically with problem size, depending on the puzzle.\\n  - Performance correlates negatively with compositional depth, but this relationship varies across puzzle types.\\n- **Open Questions:**\\n  - Why do models reduce reasoning effort at high complexity?\\n  - Can models generate solutions with explicit algorithms effectively?\\n  - Are current evaluation paradigms sufficient to understand reasoning capabilities?\\n\\n### Conclusions:\\n- **Fundamental Limitations:** Despite sophisticated self-reflection mechanisms, current LRMs do not exhibit robust, generalizable reasoning beyond moderate complexity.\\n- **Scaling Barriers:** There are inherent scaling limits in reasoning effort and accuracy, with models failing to utilize additional compute effectively at high complexity.\\n- **Implications:** The findings challenge assumptions about the reasoning capabilities of LRMs and suggest the need for new approaches, including better evaluation methods and possibly hybrid symbolic-neural systems.\\n- **Future Directions:** Further research is needed to understand the symbolic manipulation capabilities, improve reasoning robustness, and explore environments that enable controlled experimentation.\\n\\n### Limitations:\\n- The study is limited to controlled puzzle environments, which may not fully capture real-world reasoning complexity.\\n- Use of black-box API models restricts internal analysis.\\n- Precise validation relies on deterministic puzzle simulators, which may not generalize to less structured domains.\\n\\nThis work provides critical insights into the current state of reasoning models, highlighting their strengths, weaknesses, and fundamental barriers to scalable, general reasoning.'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecb229d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The document titled \"The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity\" presents a comprehensive analysis of Large Reasoning Models (LRMs) and their reasoning capabilities across various puzzle environments. The key points, important details, and conclusions are summarized below:\\n\\n### Key Points:\\n- **Objective:** To systematically investigate the reasoning capabilities and limitations of frontier LRMs using controllable puzzle environments that allow precise manipulation of problem complexity.\\n- **Methodology:** Use of four puzzle environments (Tower of Hanoi, Checker Jumping, River Crossing, Blocks World) to evaluate models\\' reasoning processes, internal traces, and performance across different complexity levels.\\n- **Models Analyzed:** Several models including Claude-3.7-Sonnet (thinking/non-thinking), DeepSeek-R1/V3, and o3-mini, with access to reasoning traces.\\n- **Evaluation Approach:** Focus on both final answer accuracy and internal reasoning traces, including solution correctness, reasoning effort, and failure analysis.\\n\\n### Important Details:\\n- **Findings on Performance:**\\n  - LRMs fail to develop generalizable reasoning beyond certain complexity thresholds, with performance collapsing to zero at high complexity.\\n  - A counterintuitive scaling limit was observed: reasoning effort (tokens used for thinking) initially increases with complexity but then decreases despite increasing problem difficulty.\\n  - Three reasoning regimes identified:\\n    1. Low complexity: Non-thinking models outperform reasoning models.\\n    2. Medium complexity: Reasoning models show an advantage.\\n    3. High complexity: Both models collapse.\\n- **Analysis of Reasoning Traces:**\\n  - Models often overthink in simple problems, exploring incorrect solutions early.\\n  - In moderate problems, correct solutions emerge later, indicating extensive exploration.\\n  - In complex problems, models fail to find correct solutions, often fixating on early wrong answers.\\n- **Limitations in Exact Computation:**\\n  - LRMs struggle with explicit algorithms and exhibit inconsistent reasoning across puzzles.\\n  - Providing explicit algorithms (e.g., Tower of Hanoi solution) does not significantly improve performance.\\n- **Behavioral Insights:**\\n  - Models show non-monotonic failure patterns, with errors occurring at different points in the solution sequence depending on problem size.\\n  - Failure move distributions suggest models are more unstable and prone to inconsistent reasoning at higher complexities.\\n- **Scaling and Complexity:**\\n  - Compositional depth (number of moves) scales exponentially or quadratically with problem size, depending on the puzzle.\\n  - Performance correlates negatively with compositional depth, but this relationship varies across puzzle types.\\n- **Open Questions:**\\n  - Why do models reduce reasoning effort at high complexity?\\n  - Can models generate solutions with explicit algorithms effectively?\\n  - Are current evaluation paradigms sufficient to understand reasoning capabilities?\\n\\n### Conclusions:\\n- **Fundamental Limitations:** Despite sophisticated self-reflection mechanisms, current LRMs do not exhibit robust, generalizable reasoning beyond moderate complexity.\\n- **Scaling Barriers:** There are inherent scaling limits in reasoning effort and accuracy, with models failing to utilize additional compute effectively at high complexity.\\n- **Implications:** The findings challenge assumptions about the reasoning capabilities of LRMs and suggest the need for new approaches, including better evaluation methods and possibly hybrid symbolic-neural systems.\\n- **Future Directions:** Further research is needed to understand the symbolic manipulation capabilities, improve reasoning robustness, and explore environments that enable controlled experimentation.\\n\\n### Limitations:\\n- The study is limited to controlled puzzle environments, which may not fully capture real-world reasoning complexity.\\n- Use of black-box API models restricts internal analysis.\\n- Precise validation relies on deterministic puzzle simulators, which may not generalize to less structured domains.\\n\\nThis work provides critical insights into the current state of reasoning models, highlighting their strengths, weaknesses, and fundamental barriers to scalable, general reasoning.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34a40217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## PDF Summary\n",
       "\n",
       "The document titled \"The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity\" presents a comprehensive analysis of Large Reasoning Models (LRMs) and their reasoning capabilities across various puzzle environments. The key points, important details, and conclusions are summarized below:\n",
       "\n",
       "### Key Points:\n",
       "- **Objective:** To systematically investigate the reasoning capabilities and limitations of frontier LRMs using controllable puzzle environments that allow precise manipulation of problem complexity.\n",
       "- **Methodology:** Use of four puzzle environments (Tower of Hanoi, Checker Jumping, River Crossing, Blocks World) to evaluate models' reasoning processes, internal traces, and performance across different complexity levels.\n",
       "- **Models Analyzed:** Several models including Claude-3.7-Sonnet (thinking/non-thinking), DeepSeek-R1/V3, and o3-mini, with access to reasoning traces.\n",
       "- **Evaluation Approach:** Focus on both final answer accuracy and internal reasoning traces, including solution correctness, reasoning effort, and failure analysis.\n",
       "\n",
       "### Important Details:\n",
       "- **Findings on Performance:**\n",
       "  - LRMs fail to develop generalizable reasoning beyond certain complexity thresholds, with performance collapsing to zero at high complexity.\n",
       "  - A counterintuitive scaling limit was observed: reasoning effort (tokens used for thinking) initially increases with complexity but then decreases despite increasing problem difficulty.\n",
       "  - Three reasoning regimes identified:\n",
       "    1. Low complexity: Non-thinking models outperform reasoning models.\n",
       "    2. Medium complexity: Reasoning models show an advantage.\n",
       "    3. High complexity: Both models collapse.\n",
       "- **Analysis of Reasoning Traces:**\n",
       "  - Models often overthink in simple problems, exploring incorrect solutions early.\n",
       "  - In moderate problems, correct solutions emerge later, indicating extensive exploration.\n",
       "  - In complex problems, models fail to find correct solutions, often fixating on early wrong answers.\n",
       "- **Limitations in Exact Computation:**\n",
       "  - LRMs struggle with explicit algorithms and exhibit inconsistent reasoning across puzzles.\n",
       "  - Providing explicit algorithms (e.g., Tower of Hanoi solution) does not significantly improve performance.\n",
       "- **Behavioral Insights:**\n",
       "  - Models show non-monotonic failure patterns, with errors occurring at different points in the solution sequence depending on problem size.\n",
       "  - Failure move distributions suggest models are more unstable and prone to inconsistent reasoning at higher complexities.\n",
       "- **Scaling and Complexity:**\n",
       "  - Compositional depth (number of moves) scales exponentially or quadratically with problem size, depending on the puzzle.\n",
       "  - Performance correlates negatively with compositional depth, but this relationship varies across puzzle types.\n",
       "- **Open Questions:**\n",
       "  - Why do models reduce reasoning effort at high complexity?\n",
       "  - Can models generate solutions with explicit algorithms effectively?\n",
       "  - Are current evaluation paradigms sufficient to understand reasoning capabilities?\n",
       "\n",
       "### Conclusions:\n",
       "- **Fundamental Limitations:** Despite sophisticated self-reflection mechanisms, current LRMs do not exhibit robust, generalizable reasoning beyond moderate complexity.\n",
       "- **Scaling Barriers:** There are inherent scaling limits in reasoning effort and accuracy, with models failing to utilize additional compute effectively at high complexity.\n",
       "- **Implications:** The findings challenge assumptions about the reasoning capabilities of LRMs and suggest the need for new approaches, including better evaluation methods and possibly hybrid symbolic-neural systems.\n",
       "- **Future Directions:** Further research is needed to understand the symbolic manipulation capabilities, improve reasoning robustness, and explore environments that enable controlled experimentation.\n",
       "\n",
       "### Limitations:\n",
       "- The study is limited to controlled puzzle environments, which may not fully capture real-world reasoning complexity.\n",
       "- Use of black-box API models restricts internal analysis.\n",
       "- Precise validation relies on deterministic puzzle simulators, which may not generalize to less structured domains.\n",
       "\n",
       "This work provides critical insights into the current state of reasoning models, highlighting their strengths, weaknesses, and fundamental barriers to scalable, general reasoning."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(f\"## PDF Summary\\n\\n{pdf_summary['summary']}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9815338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This paper, \"The Illusion of Thinking,\" investigates the fundamental strengths and limitations of Large Reasoning Models (LRMs) like Claude 3.7 Sonnet Thinking and DeepSeek-R1, particularly how their reasoning capabilities scale with problem complexity.\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "1.  **Controllable Evaluation:** The authors introduce a novel evaluation paradigm using four controllable puzzle environments (Tower of Hanoi, Checker Jumping, River Crossing, Blocks World). These allow precise manipulation of compositional complexity and detailed analysis of internal reasoning traces, circumventing data contamination issues common in standard benchmarks.\n",
      "2.  **Accuracy Collapse:** State-of-the-art LRMs exhibit a complete accuracy collapse beyond certain complexity thresholds across all puzzle environments, indicating a lack of generalizable problem-solving capabilities.\n",
      "3.  **Counter-intuitive Scaling Limit in Reasoning Effort:** LRMs' reasoning effort (measured by inference tokens) initially increases with problem complexity but then *declines* sharply as problems approach the critical collapse point, despite having ample token budget. This suggests a fundamental scaling limitation.\n",
      "4.  **Three Reasoning Regimes:**\n",
      "    *   **Low Complexity:** Standard (non-thinking) LLMs often surprisingly outperform LRMs and are more token-efficient.\n",
      "    *   **Medium Complexity:** LRMs demonstrate an advantage due to their detailed thinking processes.\n",
      "    *   **High Complexity:** Both thinking and non-thinking models experience complete performance collapse.\n",
      "\n",
      "**Important Details:**\n",
      "\n",
      "*   **Analysis of Reasoning Traces:**\n",
      "    *   **\"Overthinking\" (Low Complexity):** For simpler problems, LRMs often identify correct solutions early but then inefficiently continue exploring incorrect alternatives.\n",
      "    *   **Moderate Complexity:** Correct solutions emerge later in the thinking process, often after extensive exploration of incorrect paths.\n",
      "    *   **High Complexity:** Models consistently fail to find any correct solutions.\n",
      "*   **Limitations in Exact Computation:**\n",
      "    *   Providing explicit algorithms (e.g., for Tower of Hanoi) in the prompt does not significantly improve LRM performance, suggesting limitations in consistent logical step execution and verification, not just solution discovery.\n",
      "    *   Models show inconsistent reasoning across puzzle types; for instance, Claude 3.7 Sonnet performs well on Tower of Hanoi (N=5, 31 moves) but fails early on River Crossing (N=3, 11 moves), implying sensitivity to training data distribution.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "The study concludes that despite sophisticated self-reflection mechanisms, current LRMs possess fundamental limitations in generalizable reasoning, exhibit counter-intuitive scaling behaviors, and struggle with exact computation and consistent logical execution. These findings challenge prevailing assumptions about LRM capabilities and underscore the need for new approaches and evaluation paradigms to advance towards more robust artificial intelligence.\n"
     ]
    }
   ],
   "source": [
    "## google\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import httpx\n",
    "\n",
    "gclient = genai.Client()\n",
    "\n",
    "# Retrieve and encode the PDF byte\n",
    "doc_data = httpx.get(url).content\n",
    "\n",
    "prompt = \"\"\"You are a concise document summariser. Read the PDF at the provided URL directly and\n",
    "            return a clear, structured summary with key points, important details, and conclusions.\"\"\"\n",
    "response = gclient.models.generate_content(\n",
    "  model=\"gemini-2.5-flash\",\n",
    "  contents=[\n",
    "      types.Part.from_bytes(\n",
    "        data=doc_data,\n",
    "        mime_type='application/pdf',\n",
    "      ),\n",
    "      prompt])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8497ee56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Gemini PDF Summary\n",
       "\n",
       "This paper, \"The Illusion of Thinking,\" investigates the fundamental strengths and limitations of Large Reasoning Models (LRMs) like Claude 3.7 Sonnet Thinking and DeepSeek-R1, particularly how their reasoning capabilities scale with problem complexity.\n",
       "\n",
       "**Key Points:**\n",
       "\n",
       "1.  **Controllable Evaluation:** The authors introduce a novel evaluation paradigm using four controllable puzzle environments (Tower of Hanoi, Checker Jumping, River Crossing, Blocks World). These allow precise manipulation of compositional complexity and detailed analysis of internal reasoning traces, circumventing data contamination issues common in standard benchmarks.\n",
       "2.  **Accuracy Collapse:** State-of-the-art LRMs exhibit a complete accuracy collapse beyond certain complexity thresholds across all puzzle environments, indicating a lack of generalizable problem-solving capabilities.\n",
       "3.  **Counter-intuitive Scaling Limit in Reasoning Effort:** LRMs' reasoning effort (measured by inference tokens) initially increases with problem complexity but then *declines* sharply as problems approach the critical collapse point, despite having ample token budget. This suggests a fundamental scaling limitation.\n",
       "4.  **Three Reasoning Regimes:**\n",
       "    *   **Low Complexity:** Standard (non-thinking) LLMs often surprisingly outperform LRMs and are more token-efficient.\n",
       "    *   **Medium Complexity:** LRMs demonstrate an advantage due to their detailed thinking processes.\n",
       "    *   **High Complexity:** Both thinking and non-thinking models experience complete performance collapse.\n",
       "\n",
       "**Important Details:**\n",
       "\n",
       "*   **Analysis of Reasoning Traces:**\n",
       "    *   **\"Overthinking\" (Low Complexity):** For simpler problems, LRMs often identify correct solutions early but then inefficiently continue exploring incorrect alternatives.\n",
       "    *   **Moderate Complexity:** Correct solutions emerge later in the thinking process, often after extensive exploration of incorrect paths.\n",
       "    *   **High Complexity:** Models consistently fail to find any correct solutions.\n",
       "*   **Limitations in Exact Computation:**\n",
       "    *   Providing explicit algorithms (e.g., for Tower of Hanoi) in the prompt does not significantly improve LRM performance, suggesting limitations in consistent logical step execution and verification, not just solution discovery.\n",
       "    *   Models show inconsistent reasoning across puzzle types; for instance, Claude 3.7 Sonnet performs well on Tower of Hanoi (N=5, 31 moves) but fails early on River Crossing (N=3, 11 moves), implying sensitivity to training data distribution.\n",
       "\n",
       "**Conclusion:**\n",
       "\n",
       "The study concludes that despite sophisticated self-reflection mechanisms, current LRMs possess fundamental limitations in generalizable reasoning, exhibit counter-intuitive scaling behaviors, and struggle with exact computation and consistent logical execution. These findings challenge prevailing assumptions about LRM capabilities and underscore the need for new approaches and evaluation paradigms to advance towards more robust artificial intelligence."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"## Gemini PDF Summary\\n\\n{response.text}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2730ff16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
